{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.decomposition import PCA,  IncrementalPCA\n",
    "import math\n",
    "from sklearn import svm\n",
    "from decimal import Decimal\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iauc(P,Q):\n",
    "    return 1- np.sum(np.min(P,Q))/np.sum(P)\n",
    "def kl(p, q):\n",
    "    \"\"\"Kullback-Leibler divergence D(P || Q) for discrete distributions\n",
    "    Parameters\n",
    "    ----------\n",
    "    p, q : array-like, dtype=float, shape=n\n",
    "    Discrete probability distributions.\n",
    "    \"\"\"\n",
    "    epsilon = 0.00001\n",
    "    p = p +epsilon \n",
    "    p = p/np.sum(p)\n",
    "    q = q + epsilon\n",
    "    q = q/np.sum(q)\n",
    "\n",
    "    return entropy(p, q)\n",
    "def mkl(p,q):\n",
    "    return np.max([kl(p,q), kl(q,p)])\n",
    "def kl2(P,Q):\n",
    "    \"\"\" Epsilon is used here to avoid conditional code for checking that neither P nor Q is equal to 0. \"\"\"\n",
    "    epsilon = 0.00001\n",
    "    \n",
    "     # You may want to instead make copies to avoid changing the np arrays.\n",
    "    P = P+epsilon\n",
    "    P = P/np.sum(P)\n",
    "    Q = Q+epsilon\n",
    "    Q = Q/np.sum(Q)\n",
    "    \n",
    "\n",
    "    divergence = np.sum(P*np.log(P/Q))\n",
    "    divergence = 0\n",
    "    for i in range(len(P)):\n",
    "        \n",
    "        divergence += np.max([P[i] * math.log(P[i]/Q[i]), Q[i]* math.log(Q[i]/P[i])])\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groud_truth = []\n",
    "def generate_dataset(distribution_type, dim, length_chunk, mean, cov, change_value, change_type,change_param, num_chunk):\n",
    "    global groud_truth\n",
    "    out = []\n",
    "    if distribution_type == \"gauss\":\n",
    "        for i in range(num_chunk):\n",
    "            generated_length_chunk = length_chunk\n",
    "            \n",
    "            #generated_length_chunk = length_chunk\n",
    "            x = np.random.multivariate_normal(mean, cov, generated_length_chunk)\n",
    "\n",
    "            if change_param == \"mean\":\n",
    "                if change_type == \"sudden\":\n",
    "                    for j  in range(len(mean)):\n",
    "                        \n",
    "                            mean[j] += change_value\n",
    "                \n",
    "                elif change_type == \"incremental\":\n",
    "                    incremental_x = []\n",
    "                    for t in range(5):\n",
    "                        for j in range(len(mean)):\n",
    "                            mean[j] += change_value/5\n",
    "                        part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                        for xi in part_x:\n",
    "                            incremental_x.append(xi)\n",
    "                elif change_type == \"gradual\":\n",
    "                    gradual_x = []\n",
    "                    new_mean = []\n",
    "                    for j in range(len(mean)):\n",
    "                        new_mean.append(mean[j] + change_value)\n",
    "                    \n",
    "                    for t in range(5):\n",
    "                        threshold = (t+1)*10/5\n",
    "                        r = random.randint(0,10)\n",
    "                        if r < threshold:\n",
    "                            part_x = np.random.multivariate_normal(new_mean, cov, 1000)\n",
    "                        else:\n",
    "                            part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                        for xi in part_x:\n",
    "                            gradual_x.append(xi)\n",
    "                    mean = new_mean\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "            elif change_param == \"variance\":\n",
    "                if change_type == \"sudden\":\n",
    "                    t = random.randint(0,2)\n",
    "                    for j in range(len(cov[0])):\n",
    "                        \n",
    "                        if t% 2 == 0:\n",
    "                            cov[j][j] = (math.sqrt(cov[j][j]) + change_value*math.sqrt(cov[j][j]))*(math.sqrt(cov[j][j]) + change_value*math.sqrt(cov[j][j]))\n",
    "                        else:\n",
    "                            cov[j][j] = (math.sqrt(cov[j][j]) - change_value*math.sqrt(cov[j][j]))*(math.sqrt(cov[j][j]) - change_value*math.sqrt(cov[j][j]))\n",
    "                        for k in range(len(cov)):\n",
    "                            if k != j and k < j:\n",
    "                                cov[j][k] = 0.5 * cov[j][j]\n",
    "                                cov[k][j] = cov[j][k]\n",
    "                        \n",
    "                    print(cov)\n",
    "                elif change_type == \"incremental\":\n",
    "                    incremental_x = []\n",
    "                    t = random.randint(0,2)\n",
    "                    for t_ in range(5):\n",
    "                        for j in range(len(cov[0])):\n",
    "                            \n",
    "                            if t% 2 == 0:\n",
    "                                cov[j][j] = (math.sqrt(cov[j][j]) + change_value*math.sqrt(cov[j][j])/5)*(math.sqrt(cov[j][j]) + change_value*math.sqrt(cov[j][j])/5)\n",
    "                            else:\n",
    "                                cov[j][j] = (math.sqrt(cov[j][j]) - change_value*math.sqrt(cov[j][j])/5)*(math.sqrt(cov[j][j]) - change_value*math.sqrt(cov[j][j])/5)\n",
    "                            for k in range(len(cov)):\n",
    "                                if k != j and k < j:\n",
    "                                    cov[j][k] = 0.5 * cov[j][j]\n",
    "                                    cov[k][j] = cov[j][k]\n",
    "                        part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                        for xi in part_x:\n",
    "                            incremental_x.append(xi)\n",
    "                        #print(cov)\n",
    "                elif change_type == \"gradual\":\n",
    "                    gradual_x = []\n",
    "                    new_cov = cov\n",
    "                    t = random.randint(0,2)\n",
    "                    for j in range(len(new_cov[0])):\n",
    "                        \n",
    "                        if t% 2 == 0:\n",
    "                            new_cov[j][j] = (math.sqrt(new_cov[j][j]) + change_value*math.sqrt(new_cov[j][j]))*(math.sqrt(new_cov[j][j]) + change_value*math.sqrt(new_cov[j][j]))\n",
    "                        else:\n",
    "                            new_cov[j][j] = (math.sqrt(new_cov[j][j]) - change_value*math.sqrt(new_cov[j][j]))*(math.sqrt(new_cov[j][j]) - change_value*math.sqrt(new_cov[j][j]))\n",
    "                        for k in range(len(cov)):\n",
    "                            if k != j and k < j:\n",
    "                                new_cov[j][k] = 0.5 * new_cov[j][j]\n",
    "                                new_cov[k][j] = new_cov[j][k]\n",
    "                    for t_ in range(5):\n",
    "                        threshold = (t_+1)/5*10\n",
    "                        r = random.randint(0,10)\n",
    "                        if r < threshold:\n",
    "                            part_x = np.random.multivariate_normal(mean, new_cov, 1000)\n",
    "                        else:\n",
    "                            part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                            \n",
    "                        for xi in part_x:\n",
    "                            gradual_x.append(xi)\n",
    "                    #print(new_cov)\n",
    "                    cov = new_cov\n",
    "                                \n",
    "                                \n",
    "            elif change_param ==\"correlation\":\n",
    "                if change_type == \"sudden\":\n",
    "                    for j in range(len(cov[0])):\n",
    "                        for k in range(len(cov[0])):\n",
    "                            if  j !=k and k < j:\n",
    "                                if cov[j][k]+change_value * cov[j][j] < cov[j][j]:\n",
    "                                    cov[j][k] += change_value * cov[j][j]\n",
    "                                    cov[k][j] = cov[j][k]\n",
    "                                else :\n",
    "                                    cov[j][k] = 0.5 * cov[j][j]\n",
    "                                    cov[k][j] = cov[j][k]\n",
    "                    #print(cov)\n",
    "            #x = x.reshape(int(len(x)/2),2)\n",
    "                elif change_type == \"incremental\":\n",
    "                    incremental_x = []\n",
    "                    for t in range(5):\n",
    "                        for j in range(len(cov[0])):\n",
    "                            for k in range(len(cov[0])):\n",
    "                                if  j !=k and k < j:\n",
    "                                    if cov[j][k]+change_value/5 * cov[j][j] < cov[j][j]:\n",
    "                                        cov[j][k] += change_value/5 * cov[j][j]\n",
    "                                        cov[k][j] = cov[j][k]\n",
    "                                    else :\n",
    "                                        cov[j][k] = 0.5 * cov[j][j]\n",
    "                                        cov[k][j] = cov[j][k]\n",
    "                        #print(cov)\n",
    "                        part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                        for xi in part_x:\n",
    "                            incremental_x.append(xi)\n",
    "                elif change_type == \"gradual\":\n",
    "                    gradual_x = []\n",
    "                    new_cov = cov\n",
    "                    for j in range(len(new_cov[0])):\n",
    "                        for k in range(len(new_cov[0])):\n",
    "                            if  j !=k and k < j:\n",
    "                                if new_cov[j][k]+change_value * new_cov[j][j] < new_cov[j][j]:\n",
    "                                    new_cov[j][k] += change_value * new_cov[j][j]\n",
    "                                    new_cov[k][j] = new_cov[j][k]\n",
    "                                else :\n",
    "                                    new_cov[j][k] = 0.5 * new_cov[j][j]\n",
    "                                    new_cov[k][j] = new_cov[j][k]\n",
    "                    for t_ in range(5):\n",
    "                        threshold = (t_+1)/5*10\n",
    "                        r = random.randint(0,10)\n",
    "                        if r < threshold:\n",
    "                            part_x = np.random.multivariate_normal(mean, new_cov, 1000)\n",
    "                        else:\n",
    "                            part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                            \n",
    "                        for xi in part_x:\n",
    "                            gradual_x.append(xi)\n",
    "                    #print(new_cov)\n",
    "                    cov = new_cov \n",
    "            if i > 0:\n",
    "                groud_truth.append(len(out))\n",
    "            if change_type == \"sudden\" or len(out) == 0:\n",
    "                for xi in x:\n",
    "                    out.append(xi)\n",
    "            elif change_type == \"incremental\":\n",
    "                for xi in x[5000:]:\n",
    "                    out.append(xi)\n",
    "                for xi in incremental_x:\n",
    "                    out.append(xi)\n",
    "            elif change_type == \"gradual\":\n",
    "                for xi in x[5000:]:\n",
    "                    out.append(xi)\n",
    "                for xi in gradual_x:\n",
    "                    out.append(xi)\n",
    "            \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groud_truth = []\n",
    "true_outliers = []\n",
    "def generate_dataset_with_changes_and_outliers(distribution_type, dim, length_chunk, mean, cov, change_value, change_type,change_param, num_chunk, outlier_rate):\n",
    "    global groud_truth\n",
    "    global true_outliers\n",
    "    out = []\n",
    "    if distribution_type == \"gauss\":\n",
    "        for i in range(num_chunk):\n",
    "            generated_length_chunk = random.randint(length_chunk,length_chunk * 5)\n",
    "            \n",
    "            generated_length_chunk = length_chunk\n",
    "            x = np.random.multivariate_normal(mean, cov, generated_length_chunk)\n",
    "\n",
    "            if change_param == \"mean\":\n",
    "                if change_type == \"sudden\":\n",
    "                    for j  in range(len(mean)):\n",
    "                        \n",
    "                            mean[j] += change_value\n",
    "                \n",
    "                elif change_type == \"incremental\":\n",
    "                    incremental_x = []\n",
    "                    for t in range(5):\n",
    "                        for j in range(len(mean)):\n",
    "                            mean[j] += change_value/5\n",
    "                        part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                        for xi in part_x:\n",
    "                            incremental_x.append(xi)\n",
    "                elif change_type == \"gradual\":\n",
    "                    gradual_x = []\n",
    "                    new_mean = []\n",
    "                    for j in range(len(mean)):\n",
    "                        new_mean.append(mean[j] + change_value)\n",
    "                    \n",
    "                    for t in range(5):\n",
    "                        threshold = (t+1)*10/5\n",
    "                        r = random.randint(0,10)\n",
    "                        if r < threshold:\n",
    "                            part_x = np.random.multivariate_normal(new_mean, cov, 1000)\n",
    "                        else:\n",
    "                            part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                        for xi in part_x:\n",
    "                            gradual_x.append(xi)\n",
    "                    mean = new_mean\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "            elif change_param == \"variance\":\n",
    "                if change_type == \"sudden\":\n",
    "                    t = random.randint(0,2)\n",
    "                    for j in range(len(cov[0])):\n",
    "                        \n",
    "                        if t% 2 == 0:\n",
    "                            cov[j][j] = (math.sqrt(cov[j][j]) + change_value*math.sqrt(cov[j][j]))*(math.sqrt(cov[j][j]) + change_value*math.sqrt(cov[j][j]))\n",
    "                        else:\n",
    "                            cov[j][j] = (math.sqrt(cov[j][j]) - change_value*math.sqrt(cov[j][j]))*(math.sqrt(cov[j][j]) - change_value*math.sqrt(cov[j][j]))\n",
    "                        for k in range(len(cov)):\n",
    "                            if k != j and k < j:\n",
    "                                cov[j][k] = 0.5 * cov[j][j]\n",
    "                                cov[k][j] = cov[j][k]\n",
    "                        \n",
    "                    print(cov)\n",
    "                elif change_type == \"incremental\":\n",
    "                    incremental_x = []\n",
    "                    t = random.randint(0,2)\n",
    "                    for t_ in range(5):\n",
    "                        for j in range(len(cov[0])):\n",
    "                            \n",
    "                            if t% 2 == 0:\n",
    "                                cov[j][j] = (math.sqrt(cov[j][j]) + change_value*math.sqrt(cov[j][j])/5)*(math.sqrt(cov[j][j]) + change_value*math.sqrt(cov[j][j])/5)\n",
    "                            else:\n",
    "                                cov[j][j] = (math.sqrt(cov[j][j]) - change_value*math.sqrt(cov[j][j])/5)*(math.sqrt(cov[j][j]) - change_value*math.sqrt(cov[j][j])/5)\n",
    "                            for k in range(len(cov)):\n",
    "                                if k != j and k < j:\n",
    "                                    cov[j][k] = 0.5 * cov[j][j]\n",
    "                                    cov[k][j] = cov[j][k]\n",
    "                        part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                        for xi in part_x:\n",
    "                            incremental_x.append(xi)\n",
    "                        #print(cov)\n",
    "                elif change_type == \"gradual\":\n",
    "                    gradual_x = []\n",
    "                    new_cov = cov\n",
    "                    t = random.randint(0,2)\n",
    "                    for j in range(len(new_cov[0])):\n",
    "                        \n",
    "                        if t% 2 == 0:\n",
    "                            new_cov[j][j] = (math.sqrt(new_cov[j][j]) + change_value*math.sqrt(new_cov[j][j]))*(math.sqrt(new_cov[j][j]) + change_value*math.sqrt(new_cov[j][j]))\n",
    "                        else:\n",
    "                            new_cov[j][j] = (math.sqrt(new_cov[j][j]) - change_value*math.sqrt(new_cov[j][j]))*(math.sqrt(new_cov[j][j]) - change_value*math.sqrt(new_cov[j][j]))\n",
    "                        for k in range(len(cov)):\n",
    "                            if k != j and k < j:\n",
    "                                new_cov[j][k] = 0.5 * new_cov[j][j]\n",
    "                                new_cov[k][j] = new_cov[j][k]\n",
    "                    for t_ in range(5):\n",
    "                        threshold = (t_+1)/5*10\n",
    "                        r = random.randint(0,10)\n",
    "                        if r < threshold:\n",
    "                            part_x = np.random.multivariate_normal(mean, new_cov, 1000)\n",
    "                        else:\n",
    "                            part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                            \n",
    "                        for xi in part_x:\n",
    "                            gradual_x.append(xi)\n",
    "                    #print(new_cov)\n",
    "                    cov = new_cov\n",
    "                                \n",
    "                                \n",
    "            elif change_param ==\"correlation\":\n",
    "                if change_type == \"sudden\":\n",
    "                    for j in range(len(cov[0])):\n",
    "                        for k in range(len(cov[0])):\n",
    "                            if  j !=k and k < j:\n",
    "                                if cov[j][k]+change_value * cov[j][j] < cov[j][j]:\n",
    "                                    cov[j][k] += change_value * cov[j][j]\n",
    "                                    cov[k][j] = cov[j][k]\n",
    "                                else :\n",
    "                                    cov[j][k] = 0.5 * cov[j][j]\n",
    "                                    cov[k][j] = cov[j][k]\n",
    "                    #print(cov)\n",
    "            #x = x.reshape(int(len(x)/2),2)\n",
    "                elif change_type == \"incremental\":\n",
    "                    incremental_x = []\n",
    "                    for t in range(5):\n",
    "                        for j in range(len(cov[0])):\n",
    "                            for k in range(len(cov[0])):\n",
    "                                if  j !=k and k < j:\n",
    "                                    if cov[j][k]+change_value/5 * cov[j][j] < cov[j][j]:\n",
    "                                        cov[j][k] += change_value/5 * cov[j][j]\n",
    "                                        cov[k][j] = cov[j][k]\n",
    "                                    else :\n",
    "                                        cov[j][k] = 0.5 * cov[j][j]\n",
    "                                        cov[k][j] = cov[j][k]\n",
    "                        #print(cov)\n",
    "                        part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                        for xi in part_x:\n",
    "                            incremental_x.append(xi)\n",
    "                elif change_type == \"gradual\":\n",
    "                    gradual_x = []\n",
    "                    new_cov = cov\n",
    "                    for j in range(len(new_cov[0])):\n",
    "                        for k in range(len(new_cov[0])):\n",
    "                            if  j !=k and k < j:\n",
    "                                if new_cov[j][k]+change_value * new_cov[j][j] < new_cov[j][j]:\n",
    "                                    new_cov[j][k] += change_value * new_cov[j][j]\n",
    "                                    new_cov[k][j] = new_cov[j][k]\n",
    "                                else :\n",
    "                                    new_cov[j][k] = 0.5 * new_cov[j][j]\n",
    "                                    new_cov[k][j] = new_cov[j][k]\n",
    "                    for t_ in range(5):\n",
    "                        threshold = (t_+1)/5*10\n",
    "                        r = random.randint(0,10)\n",
    "                        if r < threshold:\n",
    "                            part_x = np.random.multivariate_normal(mean, new_cov, 1000)\n",
    "                        else:\n",
    "                            part_x = np.random.multivariate_normal(mean, cov, 1000)\n",
    "                            \n",
    "                        for xi in part_x:\n",
    "                            gradual_x.append(xi)\n",
    "                    #print(new_cov)\n",
    "                    cov = new_cov \n",
    "            if i > 0:\n",
    "                groud_truth.append(len(out))\n",
    "            if change_type == \"sudden\" or len(out) == 0:\n",
    "                for xi in x:\n",
    "                    out.append(xi)\n",
    "            elif change_type == \"incremental\":\n",
    "                for xi in x[5000:]:\n",
    "                    out.append(xi)\n",
    "                for xi in incremental_x:\n",
    "                    out.append(xi)\n",
    "            elif change_type == \"gradual\":\n",
    "                for xi in x[5000:]:\n",
    "                    out.append(xi)\n",
    "                for xi in gradual_x:\n",
    "                    out.append(xi)\n",
    "            \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_detection_AVG(data, window_size, slide_size):\n",
    "    global threshold_avg\n",
    "    threshold = threshold_avg\n",
    "    \n",
    "    global alpha\n",
    "    \n",
    "    dim = len(data[0,:])\n",
    "    #print(\"dim = \"+ str(dim))\n",
    "    start_idx = 0\n",
    "    end_idx = window_size\n",
    "    #get reference \n",
    "    reference_window = data[0:window_size,:]\n",
    "    #print(reference_window[4999])\n",
    "    ref_hists = []\n",
    "    bin_edges = []\n",
    "    num_selected_components = 0\n",
    "    \n",
    "    \n",
    "    reference_window, bin_edges, dim, pca, ref_hists,num_selected_components = get_reference_window(reference_window, dim)\n",
    "    #alpha = x1/(len(bin_edges)-1)\n",
    "    #print(\"Num bin = \" + str(len(bin_edges[0])))\n",
    "    \n",
    "    count_slide = 0\n",
    "    start_idx += slide_size\n",
    "    end_idx += slide_size\n",
    "    ref_end_idx = end_idx\n",
    "    distances = []\n",
    "    changes = []\n",
    "    \n",
    "    current_avg = 0\n",
    "    count_avg =0\n",
    "    avg_Sc = 0\n",
    "    while(end_idx < len(data)):\n",
    "        count_slide +=1\n",
    "        \n",
    "        current_window = data[start_idx: end_idx]\n",
    "        current_window = pca.transform(current_window)\n",
    "        #get current hist\n",
    "        current_hists = get_hists(current_window,num_selected_components, bin_edges)\n",
    "        #distance = distance2(current_hists,ref_hists, pca)\n",
    "        distance = distance_max2(ref_hists,current_hists, pca)\n",
    "        #distance = distance_min(current_hists,ref_hists, pca)\n",
    "        #distance = distance_sum(current_hists,ref_hists, pca)\n",
    "        #distance = distance_sum_square(current_hists,ref_hists, pca)\n",
    "        if count_slide %50000 == 0:\n",
    "            #print(\"Count slide = \"+ str(count_slide))\n",
    "            #print(\"distance = \"+ str(distance))\n",
    "            pass\n",
    "        if start_idx > ref_end_idx:\n",
    "            distances.append(distance)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(distances) > window_size/slide_size:\n",
    "            del distances[0]\n",
    "            current_avg = np.sum(distances)/len(distances)\n",
    "            avg_Sc = (avg_Sc * count_avg + current_avg) /(count_avg +1)\n",
    "            count_avg +=1\n",
    "        #print(distance)\n",
    "        if start_idx > ref_end_idx:\n",
    "            if start_idx > ref_end_idx and avg_Sc > 0 and current_avg/avg_Sc > threshold:\n",
    "                #print(\"Change at \"+ str(end_idx))\n",
    "                changes.append(end_idx)\n",
    "                #update reference\n",
    "                start_idx = end_idx\n",
    "                end_idx = start_idx + window_size\n",
    "                reference_window, bin_edges, dim, pca , ref_hists, num_selected_components= get_reference_window(data[start_idx: end_idx], dim)\n",
    "                ref_end_idx = end_idx\n",
    "                current_avg = 0\n",
    "                distances = []\n",
    "        \n",
    "        start_idx += slide_size\n",
    "        end_idx += slide_size\n",
    "        #lis_sequence.append(lis)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #fig = plt.figure()\n",
    "    #plt.plot(lis_sequence)\n",
    "    #fig.show()\n",
    "    \n",
    "    return changes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncating_number = 3\n",
    "def change_detection_truncate(data, window_size, slide_size):\n",
    "    threshold = 2*math.sqrt(window_size/slide_size)\n",
    "    \n",
    "    #threshold = window_size/slide_size*0.5\n",
    "    global truncating_numbercompute_score\n",
    "    global alpha\n",
    "    \n",
    "    dim = len(data[0,:])\n",
    "    #print(\"dim = \"+ str(dim))\n",
    "    start_idx = 0\n",
    "    end_idx = window_size\n",
    "    #get reference \n",
    "    reference_window = data[0:window_size,:]\n",
    "    #print(reference_window[4999])\n",
    "    ref_hists = []\n",
    "    bin_edges = []\n",
    "    num_selected_components = 0\n",
    "    \n",
    "    \n",
    "    reference_window, bin_edges, dim, pca, ref_hists,num_selected_components = get_reference_window(reference_window, dim)\n",
    "    #alpha = x1/(len(bin_edges)-1)\n",
    "    #print(\"Num bin = \" + str(len(bin_edges[0])))\n",
    "    \n",
    "    count_slide = 0\n",
    "    start_idx += slide_size\n",
    "    end_idx += slide_size\n",
    "    ref_end_idx = end_idx\n",
    "    distances = []\n",
    "    lis_sequence = []\n",
    "    changes = []\n",
    "    \n",
    "    possible_lis = 0\n",
    "    \n",
    "    while(end_idx < len(data)):\n",
    "        count_slide +=1\n",
    "        \n",
    "        current_window = data[start_idx: end_idx]\n",
    "        #current_window = pca.transform(preprocessing.normalize(current_window, norm='l2'))\n",
    "        current_window = pca.transform(current_window)\n",
    "        #get current hist\n",
    "        current_hists = get_hists(current_window,num_selected_components, bin_edges)\n",
    "        #distance = distance2(current_hists,ref_hists, pca)\n",
    "        distance = distance_max2(current_hists,ref_hists,pca)\n",
    "        #distance = distance_min2(current_hists,ref_hists,pca)\n",
    "        distance = round (Decimal(distance),truncating_number)\n",
    "        #distance = distance_min(current_hists,ref_hists, pca)\n",
    "        #distance = distance_sum(current_hists,ref_hists, pca)\n",
    "        #distance = distance_sum_square(current_hists,ref_hists, pca)\n",
    "        if count_slide %50000 == 0:\n",
    "            \n",
    "            print(\"Count slide = \"+ str(count_slide))\n",
    "            #print(\"distance = \"+ str(distance))\n",
    "        if start_idx > ref_end_idx:\n",
    "            distances.append(distance)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(distances) > window_size/slide_size:\n",
    "            del distances[0]\n",
    "        #print(distance)\n",
    "        if start_idx > ref_end_idx:\n",
    "            possible_lis +=1 \n",
    "        \n",
    "        if possible_lis > threshold:\n",
    "            lis = len(subsequence(distances))\n",
    "            possible_lis = lis\n",
    "            if start_idx > ref_end_idx and lis > threshold:\n",
    "                #print(\"Change at \"+ str(end_idx))\n",
    "                changes.append(end_idx)\n",
    "                #update reference\n",
    "                #start_idx = end_idx\n",
    "                #end_idx = start_idx + window_size\n",
    "                reference_window, bin_edges, dim, pca , ref_hists, num_selected_components= get_reference_window(data[start_idx: end_idx], dim)\n",
    "                ref_end_idx = end_idx\n",
    "                possible_lis = 0\n",
    "                distances = []\n",
    "        \n",
    "        start_idx += slide_size\n",
    "        end_idx += slide_size\n",
    "        #lis_sequence.append(lis)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #fig = plt.figure()\n",
    "    #plt.plot(lis_sequence)\n",
    "    #fig.show()\n",
    "    \n",
    "    return changes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'get_reference_window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-05a7d0053627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gauss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m#dlis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mchanges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_detection_truncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mw_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgroud_truth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchanges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_p\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw_r\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-33375212750b>\u001b[0m in \u001b[0;36mchange_detection_truncate\u001b[0;34m(data, window_size, slide_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mreference_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_hists\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_selected_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reference_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m#alpha = x1/(len(bin_edges)-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#print(\"Num bin = \" + str(len(bin_edges[0])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'get_reference_window' is not defined"
     ]
    }
   ],
   "source": [
    "change_types = [\"sudden\", \"incremental\", \"gradual\"]\n",
    "change_params = [ \"mean\",\n",
    "                 \"variance\"\n",
    "                 , \"correlation\"\n",
    "]\n",
    "change_values = {\"mean\":[0.03], \"variance\":[0.2],\n",
    "                 \"correlation\":[ 0.1]}\n",
    "slide_sizes = [1,20,100,200,400]\n",
    "\n",
    "\n",
    "\n",
    "truncating_number = 3\n",
    "threshold_avg = 1.3\n",
    "Sigma = 500\n",
    "delta = 0.005\n",
    "#vary change value, sudden\n",
    "for change_param in change_params:\n",
    "    change_type = \"sudden\"\n",
    "    \n",
    "    for slide_size in slide_sizes:\n",
    "        for change_value in change_values[change_param]:\n",
    "            w_precisions = {\"dlis\":[], \"avg\":[], \"phdt\":[]}\n",
    "            w_recalls = {\"dlis\":[], \"avg\":[], \"phdt\":[]}\n",
    "            w_f1s = {\"dlis\":[], \"avg\":[], \"phdt\":[]}\n",
    "            for time_ in range(1):\n",
    "                #create data set\n",
    "                mean = [0.01, 0.01]\n",
    "                cov = [[0.04, 0.02],[0.02, 0.04]]\n",
    "                groud_truth = []\n",
    "                d = generate_dataset(\"gauss\",2, 50000,mean, cov, change_value, change_type, change_param, 20)\n",
    "                #dlis\n",
    "                changes = change_detection_truncate(d, 10000, 20)\n",
    "                w_p, w_r, w_f = compute_score( groud_truth,changes,decay_factor=0.1,window_size=10000)\n",
    "                if not math.isnan(w_p+w_r+w_f):\n",
    "                    w_precisions[\"dlis\"].append(w_p)\n",
    "                    w_recalls[\"dlis\"].append(w_r)\n",
    "                    w_f1s[\"dlis\"].append(w_f)\n",
    "                #avg\n",
    "                changes = change_detection_AVG(d, 10000, 20)\n",
    "                w_p, w_r, w_f = compute_score( groud_truth,changes,decay_factor=0.1,window_size=10000)\n",
    "                if not math.isnan(w_p+w_r+w_f):\n",
    "                    w_precisions[\"avg\"].append(w_p)\n",
    "                    w_recalls[\"avg\"].append(w_r)\n",
    "                    w_f1s[\"avg\"].append(w_f)\n",
    "                #phdt\n",
    "                changes = change_detection_PHDT(d, 10000, 20)\n",
    "                w_p, w_r, w_f = compute_score( groud_truth,changes,decay_factor=0.1,window_size=10000)\n",
    "                if not math.isnan(w_p+w_r+w_f):\n",
    "                    w_precisions[\"phdt\"].append(w_p)\n",
    "                    w_recalls[\"phdt\"].append(w_r)\n",
    "                    w_f1s[\"phdt\"].append(w_f)\n",
    "                #print(w_precisions)\n",
    "                #print(w_recalls)\n",
    "                #print(w_f1s)\n",
    "            #print results\n",
    "            print(\"change \"+change_param +\", slide size = \" +str(slide_size) + \"change value = \"+str(change_value))\n",
    "            print(\"DLIS\")\n",
    "            print(str(np.sum(w_precisions[\"dlis\"])/len(w_precisions[\"dlis\"])) +\"-\"+\n",
    "                  str(np.sum(w_recalls[\"dlis\"])/len(w_recalls[\"dlis\"])) +\"-\"+str(np.sum(w_f1s[\"dlis\"])/len(w_f1s[\"dlis\"])) )\n",
    "            print(\"AVG\")\n",
    "            print(str(np.sum(w_precisions[\"avg\"])/len(w_precisions[\"avg\"])) +\"-\"+\n",
    "                  str(np.sum(w_recalls[\"avg\"])/len(w_recalls[\"avg\"])) +\"-\"+str(np.sum(w_f1s[\"avg\"])/len(w_f1s[\"avg\"])) )\n",
    "            print(\"PHDT\")\n",
    "            print(str(np.sum(w_precisions[\"phdt\"])/len(w_precisions[\"phdt\"])) +\"-\"+\n",
    "                  str(np.sum(w_recalls[\"phdt\"])/len(w_recalls[\"phdt\"])) +\"-\"+str(np.sum(w_f1s[\"phdt\"])/len(w_f1s[\"phdt\"])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Data name = covtype.data\n",
      "Error type = double\n",
      "[50000, 100000, 150000, 200000, 250000]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error type = double\n",
      "DLIS\n",
      "nan-nan-nan\n",
      "AVG\n",
      "nan-nan-nan\n",
      "PHDT\n",
      "nan-nan-nan\n",
      "----------------------------------------------\n",
      "Data name = ethylene.txt\n",
      "Error type = double\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:138: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:141: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5ff849720feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m#create data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m#d = generate_dataset(\"gauss\",2, 50000,mean, cov, change_value, change_type, change_param, 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mno_change\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroud_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_dataset_no_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_change_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_change\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroud_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"covtype.data\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-5ff849720feb>\u001b[0m in \u001b[0;36msample_dataset_no_change\u001b[0;34m(filename, change_type, length_chunk_, num_chunk)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mselected_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luan/.local/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# Parse each line, including the first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luan/.local/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msplit_line\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregex_comments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'\\r\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#process real_world dataset\n",
    "groud_truth = []\n",
    "selected_dims = []\n",
    "import time \n",
    "def sample_a_segment(dataset, length_chunk_):\n",
    "    return random.sample(list(dataset), length_chunk_)\n",
    "        \n",
    "def sample_dataset_no_change(filename, change_type, length_chunk_, num_chunk):\n",
    "    global groud_truth\n",
    "    global selected_dims\n",
    "    \n",
    "    dataset = np.loadtxt(filename,delimiter=\",\")\n",
    "    \n",
    "    random.seed(int(time.time()))\n",
    "    no_change = []\n",
    "    end_chunk_list = []\n",
    "    for i in range(num_chunk):\n",
    "        #length_chunk = random.randint(length_chunk_, length_chunk_*3)\n",
    "        length_chunk = length_chunk_\n",
    "        chunk = sample_a_segment(dataset, length_chunk)\n",
    "        #change in random dim\n",
    "        no_change.extend(chunk[:])\n",
    "        end_chunk_list.append(len(no_change))\n",
    "    \n",
    "    return np.array(no_change), end_chunk_list\n",
    "def add_change_to_data(no_change, end_chunk_list, change_type):\n",
    "    global selected_dims\n",
    "    global groud_truth\n",
    "    ground_truth = end_chunk_list[0:len(end_chunk_list)-1]\n",
    "    out = []\n",
    "    dim = len(no_change[0])\n",
    "    for i in range(end_chunk_list[0]):\n",
    "        out.append(no_change[i][:])\n",
    "    if change_type == \"double\":\n",
    "        for i in range(1, len(end_chunk_list)):\n",
    "            selected_dim = random.randint(0, len(no_change[0])-1)\n",
    "            while(len(selected_dims) > 0 and selected_dim==selected_dims[-1]):\n",
    "                selected_dim = random.randint(0, len(no_change[0])-1)\n",
    "            selected_dims.append(selected_dim)\n",
    "            start_idx = end_chunk_list[i-1]\n",
    "            end_idx = end_chunk_list[i]\n",
    "            for j in range(start_idx, end_idx):\n",
    "                temp_data = []\n",
    "                for k in range(dim):\n",
    "                    if k!= selected_dim:\n",
    "                        temp_data.append(no_change[j][k])\n",
    "                    else:\n",
    "                        temp_data.append(no_change[j][k] * 2)\n",
    "                out.append(temp_data)\n",
    "    elif change_type == \"gaussian\":\n",
    "        for i in range(1, len(end_chunk_list)):\n",
    "            selected_dim = random.randint(0, len(no_change[0])-1)\n",
    "            while(len(selected_dims) > 0 and selected_dim==selected_dims[-1]):\n",
    "                selected_dim = random.randint(0, len(no_change[0])-1)\n",
    "            selected_dims.append(selected_dim)\n",
    "            start_idx = end_chunk_list[i-1]\n",
    "            end_idx = end_chunk_list[i]\n",
    "            data_mean = np.mean(no_change[start_idx: end_idx][selected_dim])\n",
    "            data_std = np.std(no_change[start_idx: end_idx][selected_dim])\n",
    "            \n",
    "            \n",
    "            for j in range(start_idx, end_idx):\n",
    "                temp_data = []\n",
    "                for k in range(dim):\n",
    "                    if k!= selected_dim:\n",
    "                        temp_data.append(no_change[j][k])\n",
    "                    else:\n",
    "                        #add noise here\n",
    "                        temp_data.append(no_change[j][k]+np.random.normal(data_mean/10, data_std/10,1) )\n",
    "                out.append(temp_data)\n",
    "    return np.array(out)\n",
    "     \n",
    "        \n",
    "\n",
    "truncating_number = 3\n",
    "threshold_avg = 1.3\n",
    "Sigma = 500\n",
    "delta = 0.005\n",
    "all_result = {}\n",
    "\n",
    "\n",
    "outlier_rate = 0.05\n",
    "window = 10000\n",
    "slide = 100\n",
    "\n",
    "\n",
    "mean = [0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "cov = [[0.04, 0.02, 0.02, 0.02, 0.02],[0.02, 0.04, 0.02, 0.02, 0.02],[0.02, 0.02, 0.04, 0.02, 0.02],[0.02, 0.02, 0.02, 0.04, 0.02],[0.02, 0.02, 0.02, 0.02, 0.04]]\n",
    "\n",
    "#vary change value, sudden\n",
    "for data_name in [\"covtype.data\", \"ethylene.txt\", \"tao.txt\", \"household.txt\"]:\n",
    "    for error_type in [\"double\"]:\n",
    "        print(\"----------------------------------------------\")\n",
    "        print(\"Data name = \"+ str(data_name))\n",
    "        print(\"Error type = \"+ str(error_type))\n",
    "\n",
    "        w_precisions = {\"dlis\":[], \"avg\":[], \"phdt\":[]}\n",
    "        w_recalls = {\"dlis\":[], \"avg\":[], \"phdt\":[]}\n",
    "        w_f1s = {\"dlis\":[], \"avg\":[], \"phdt\":[]}\n",
    "        for time_ in range(1):\n",
    "            #create data set\n",
    "            #d = generate_dataset(\"gauss\",2, 50000,mean, cov, change_value, change_type, change_param, 20)\n",
    "            no_change, groud_truth = sample_dataset_no_change(data_name, error_type, 50000, 5)\n",
    "            d = add_change_to_data(no_change, groud_truth, error_type)\n",
    "            if data_name == \"covtype.data\":\n",
    "                d = d[:, :10]\n",
    "            print(groud_truth)\n",
    "            #dlis\n",
    "            changes = change_detection_truncate(d, 10000, 20)\n",
    "            print(changes)\n",
    "            w_p, w_r, w_f = compute_score( groud_truth,changes,decay_factor=0.1,window_size=10000)\n",
    "            if not math.isnan(w_p+w_r+w_f):\n",
    "                w_precisions[\"dlis\"].append(w_p)\n",
    "                w_recalls[\"dlis\"].append(w_r)\n",
    "                w_f1s[\"dlis\"].append(w_f)\n",
    "            #avg\n",
    "            changes = change_detection_AVG(d, 10000, 20)\n",
    "            w_p, w_r, w_f = compute_score( groud_truth,changes,decay_factor=0.1,window_size=10000)\n",
    "            if not math.isnan(w_p+w_r+w_f):\n",
    "                w_precisions[\"avg\"].append(w_p)\n",
    "                w_recalls[\"avg\"].append(w_r)\n",
    "                w_f1s[\"avg\"].append(w_f)\n",
    "            #phdt\n",
    "            changes = change_detection_PHDT(d, 10000, 20)\n",
    "            w_p, w_r, w_f = compute_score( groud_truth,changes,decay_factor=0.1,window_size=10000)\n",
    "            if not math.isnan(w_p+w_r+w_f):\n",
    "                w_precisions[\"phdt\"].append(w_p)\n",
    "                w_recalls[\"phdt\"].append(w_r)\n",
    "                w_f1s[\"phdt\"].append(w_f)\n",
    "            #print(w_precisions)\n",
    "            #print(w_recalls)\n",
    "            #print(w_f1s)\n",
    "        #print results\n",
    "        #print(\"change \"+change_param +\", slide size = \" +str(slide_size) + \"change value = \"+str(change_value))\n",
    "        print(\"Error type = \"+ str(error_type))\n",
    "        print(\"DLIS\")\n",
    "        print(str(np.sum(w_precisions[\"dlis\"])/len(w_precisions[\"dlis\"])) +\"-\"+\n",
    "              str(np.sum(w_recalls[\"dlis\"])/len(w_recalls[\"dlis\"])) +\"-\"+str(np.sum(w_f1s[\"dlis\"])/len(w_f1s[\"dlis\"])) )\n",
    "        print(\"AVG\")\n",
    "        print(str(np.sum(w_precisions[\"avg\"])/len(w_precisions[\"avg\"])) +\"-\"+\n",
    "              str(np.sum(w_recalls[\"avg\"])/len(w_recalls[\"avg\"])) +\"-\"+str(np.sum(w_f1s[\"avg\"])/len(w_f1s[\"avg\"])) )\n",
    "        print(\"PHDT\")\n",
    "        print(str(np.sum(w_precisions[\"phdt\"])/len(w_precisions[\"phdt\"])) +\"-\"+\n",
    "              str(np.sum(w_recalls[\"phdt\"])/len(w_recalls[\"phdt\"])) +\"-\"+str(np.sum(w_f1s[\"phdt\"])/len(w_f1s[\"phdt\"])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n",
      "[50000, 100000, 150000, 200000, 250000, 300000, 350000, 400000, 450000, 500000, 550000, 600000, 650000, 700000, 750000, 800000, 850000, 900000, 950000, 1000000, 1050000, 1100000, 1150000, 1200000, 1250000, 1300000, 1350000, 1400000, 1450000, 1500000, 1550000, 1600000, 1650000, 1700000, 1750000, 1800000, 1850000, 1900000, 1950000, 2000000, 2050000, 2100000, 2150000, 2200000, 2250000, 2300000, 2350000, 2400000, 2450000, 2500000, 2550000, 2600000, 2650000, 2700000, 2750000, 2800000, 2850000, 2900000, 2950000, 3000000, 3050000, 3100000, 3150000, 3200000, 3250000, 3300000, 3350000, 3400000, 3450000, 3500000, 3550000, 3600000, 3650000, 3700000, 3750000, 3800000, 3850000, 3900000, 3950000, 4000000, 4050000, 4100000, 4150000, 4200000, 4250000, 4300000, 4350000, 4400000, 4450000, 4500000, 4550000, 4600000, 4650000, 4700000, 4750000, 4800000, 4850000, 4900000, 4950000]\n"
     ]
    }
   ],
   "source": [
    "#process real_world dataset\n",
    "groud_truth = []\n",
    "def sample_dataset(filename, change_type, length_chunk, num_chunk):\n",
    "    global groud_truth\n",
    "    dataset = np.loadtxt(filename,delimiter=\",\")\n",
    "    #remove nan\n",
    "        #sample\n",
    "    out = []\n",
    "    for i in range(num_chunk):\n",
    "        last_dim = -1\n",
    "        chunk = []\n",
    "        for j in range(length_chunk):\n",
    "            idx = random.randint(0, len(dataset)-1)\n",
    "            data = dataset[idx]\n",
    "            chunk.append(data)\n",
    "        #select dim to change\n",
    "        selected_dim = random.randint(0, len(chunk[0])-1)\n",
    "        while(selected_dim == last_dim):\n",
    "            selected_dim = random.randint(0, len(chunk[0])-1)\n",
    "        last_dim = selected_dim\n",
    "        #change mean\n",
    "        if change_type == \"double\":\n",
    "            for j in range(length_chunk):\n",
    "                chunk[j][selected_dim] *= 2\n",
    "        #append to out\n",
    "        elif change_type == \"gauss\":\n",
    "            chunk_data =np.array(chunk)\n",
    "            mean_chunk = np.mean(chunk_data[:,selected_dim])\n",
    "            noise_mean = mean_chunk * 1/10\n",
    "            noise_std = np.std(chunk_data[:,selected_dim]) *1/10\n",
    "            noise_chunk = np.random.normal(noise_mean, noise_std, length_chunk)\n",
    "            for j in range(length_chunk):\n",
    "                chunk[j][selected_dim] += noise_chunk[j]\n",
    "        if i > 0:\n",
    "            groud_truth.append(len(out))\n",
    "        for data in chunk:\n",
    "            out.append(data)\n",
    "        \n",
    "    print(len(out))\n",
    "    print(groud_truth)\n",
    "    \n",
    "    return np.array(out)\n",
    "        \n",
    "d= sample_dataset(\"tao.txt\", \"gauss\", 50000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate high dimensional data\n",
    "dim = 5\n",
    "groud_truth = []\n",
    "\n",
    "mean = []\n",
    "cov = []\n",
    "for i in range(dim):\n",
    "    mean.append(0.5)\n",
    "    cov.append([])\n",
    "for i in range(len(cov)):\n",
    "    cov[i]= []\n",
    "    for j in range(dim):\n",
    "        cov[i].append(0)\n",
    "    for j in range(len(cov[i])):\n",
    "        if j == i:\n",
    "            cov[i][j] = 0.04\n",
    "        else:\n",
    "            cov[i][j] = 0.02\n",
    "num_change_dimension = 1\n",
    "\n",
    "def generate_dataset_high_dim(distribution_type, dim, length_chunk, mean, cov, change_value, change_type,change_param, num_chunk):\n",
    "    global num_change_dimension\n",
    "    #change mean\n",
    "    out = []\n",
    "    ground_truth = []\n",
    "    if distribution_type == \"gauss\":\n",
    "        for i in range(num_chunk):\n",
    "            generated_length_chunk = random.randint(length_chunk,length_chunk * 4)\n",
    "            x = np.random.multivariate_normal(mean, cov, generated_length_chunk)\n",
    "            if change_param == \"mean\":\n",
    "                if change_type == \"sudden\":\n",
    "                    for j  in range(num_change_dimension):\n",
    "                        mean[j] += change_value\n",
    "            \n",
    "            elif change_param == \"correlation\":\n",
    "                if change_type == \"sudden\":\n",
    "                    k = random.randint(0, dim-1)\n",
    "                    j = random.randint(0, dim-1)\n",
    "                    while j == k:\n",
    "                        j = random.randint(0, dim-1)\n",
    "                    if j != k:\n",
    "\n",
    "                        if cov[j][k]+change_value * cov[j][j] < cov[j][j]:\n",
    "                            cov[j][k] += change_value * cov[j][j]\n",
    "                            cov[k][j] = cov[j][k]\n",
    "                        else :\n",
    "                            cov[j][k] = 0.5 * cov[j][j]\n",
    "                            cov[k][j] = cov[j][k]    \n",
    "                    \n",
    "                    print(cov)\n",
    "            if i > 0:\n",
    "                groud_truth.append(len(out))\n",
    "            if change_type == \"sudden\" or len(out) == 0:\n",
    "                for xi in x:\n",
    "                    out.append(xi)\n",
    "        \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequence3(seq):\n",
    "    if not seq:\n",
    "        return seq\n",
    "\n",
    "    M = [None] * len(seq)    # offset by 1 (j -> j-1)\n",
    "    P = [None] * len(seq)\n",
    "\n",
    "    # Since we have at least one element in our list, we can start by\n",
    "    # knowing that the there's at least an increasing subsequence of length one:\n",
    "    # the first element.\n",
    "    L = 1\n",
    "    M[0] = 0\n",
    "\n",
    "    # Looping over the sequence starting from the second element\n",
    "    for i in range(1, len(seq)):\n",
    "        # Binary search: we want the largest j <= L\n",
    "        #  such that seq[M[j]] < seq[i] (default j = 0),\n",
    "        #  hence we want the lower bound at the end of the search process.\n",
    "        lower = 0\n",
    "        upper = L\n",
    "\n",
    "        # Since the binary search will not look at the upper bound value,\n",
    "        # we'll have to check that manually\n",
    "        if seq[M[upper-1]] < seq[i] and (P[M[upper-1]] == None or seq[i] > 2*seq[M[upper-1]] - seq[P[M[upper-1]]])  :\n",
    "            j = upper\n",
    "\n",
    "        else:\n",
    "            # actual binary search loop\n",
    "            while upper - lower > 1:\n",
    "                mid = (upper + lower) // 2\n",
    "                if seq[M[mid-1]] < seq[i] and (P[M[mid-1]] == None or 2*seq[M[mid-1]] - seq[P[M[mid-1]]]< seq[i]):\n",
    "                    lower = mid\n",
    "                else:\n",
    "                    upper = mid\n",
    "\n",
    "            j = lower    # this will also set the default value to 0\n",
    "\n",
    "        P[i] = M[j-1]\n",
    "\n",
    "        if j == L or seq[i] < seq[M[j]]:\n",
    "            M[j] = i\n",
    "            L = max(L, j+1)\n",
    "\n",
    "    # Building the result: [seq[M[L-1]], seq[P[M[L-1]]], seq[P[P[M[L-1]]]], ...]\n",
    "    result = []\n",
    "    pos = M[L-1]\n",
    "    for _ in range(L):\n",
    "        result.append(seq[pos])\n",
    "        pos = P[pos]\n",
    "\n",
    "    return result[::-1]    # reversing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_window(curren_window, dim):\n",
    "    ref_hists = []\n",
    "    bin_edges = []\n",
    "    num_selected_components = 0\n",
    "    \n",
    "    pca = PCA(n_components=dim)\n",
    "    #normalized_current_window = preprocessing.normalize(curren_window, norm='l2')\n",
    "    pca.fit(curren_window)\n",
    "    \n",
    "    s = 0\n",
    "    \n",
    "    for i in range(len(pca.explained_variance_ratio_)):\n",
    "        s+= pca.explained_variance_ratio_[i]\n",
    "        if s >= 0.999:\n",
    "            num_selected_components = i + 1\n",
    "            break\n",
    "    #print(\"Num selected components = \"+ str(num_selected_components))\n",
    "    reference_window = pca.transform(curren_window)\n",
    "    for i in range(num_selected_components):\n",
    "        hist, bin_edge = np.histogram(reference_window[:,i], 'auto')\n",
    "        for j in range(len(hist)):\n",
    "            hist[j] += 1\n",
    "        ref_hists.append(hist)\n",
    "        bin_edges.append(bin_edge)\n",
    "    return reference_window, bin_edges, dim, pca, ref_hists, num_selected_components\n",
    "def get_hists(current_window, dim, bin_edges):\n",
    "    current_hists = []\n",
    "    for i in range(dim):\n",
    "        hist, edge = np.histogram(current_window[:,i], bin_edges[i])\n",
    "        for j in range(len(hist)):\n",
    "            hist[j] += 1\n",
    "        current_hists.append(hist)\n",
    "    return current_hists\n",
    "def distance1(hist1, hist2):\n",
    "    dim = len(hist1)\n",
    "    dlist=  []\n",
    "    for i in range(dim):\n",
    "        d1 = mkl(hist1[i], hist2[i])\n",
    "        #d2 = kl(hist2[i], hist1[i])\n",
    "        #print(\"d = \"+str(d1))\n",
    "        #print(\"d2 = \"+str(d2))\n",
    "        dlist.append(d1)\n",
    "\n",
    "    return np.max(dlist)\n",
    "def distance2(hist1, hist2, pca):\n",
    "    dim = len(hist1)\n",
    "    dlist=  []\n",
    "    for i in range(dim):\n",
    "        d1 = kl(hist1[i], hist2[i])\n",
    "        #d2 = kl(hist2[i], hist1[i])\n",
    "        #print(\"d = \"+str(d1))W_precision = 0.9\n",
    "        #print(\"d2 = \"+str(d2))\n",
    "        dlist.append(d1)\n",
    "\n",
    "    return np.sum([dlist[i]*pca.explained_variance_ratio_[i] for i in range(len(dlist))])\n",
    "def distance_max(hist1, hist2, pca):\n",
    "    dim = len(hist1)\n",
    "    dlist=  []\n",
    "    for i in range(dim):\n",
    "        d1 = kl(hist1[i], hist2[i])\n",
    "        #d2 = kl(hist2[i], hist1[i])\n",
    "        #print(\"d = \"+str(d1))W_precision = 0.9\n",
    "        #print(\"d2 = \"+str(d2))\n",
    "        dlist.append(d1)\n",
    "\n",
    "    return np.max(dlist)\n",
    "def distance_max2(hist1, hist2, pca):\n",
    "    dim = len(hist1)\n",
    "    dlist=  []\n",
    "    for i in range(dim):\n",
    "        d1 = kl2(hist1[i], hist2[i])\n",
    "        #d2 = kl(hist2[i], hist1[i])\n",
    "        #print(\"d = \"+str(d1))W_precision = 0.9\n",
    "        #print(\"d2 = \"+str(d2))\n",
    "        dlist.append(d1)\n",
    "\n",
    "    return np.max(dlist)\n",
    "def distance_sum(hist1, hist2, pca):\n",
    "    dim = len(hist1)\n",
    "    dlist=  []\n",
    "    for i in range(dim):\n",
    "        d1 = kl(hist1[i], hist2[i])\n",
    "        #d2 = kl(hist2[i], hist1[i])\n",
    "        #print(\"d = \"+str(d1))W_precision = 0.9\n",
    "        #print(\"d2 = \"+str(d2))\n",
    "        dlist.append(d1)\n",
    "\n",
    "    return np.sum(dlist)\n",
    "def distance_sum_square(hist1, hist2, pca):\n",
    "    dim = len(hist1)\n",
    "    dlist=  []\n",
    "    for i in range(dim):\n",
    "        d1 = kl(hist1[i], hist2[i])\n",
    "        #d2 = kl(hist2[i], hist1[i])\n",
    "        #print(\"d = \"+str(d1))W_precision = 0.9\n",
    "        #print(\"d2 = \"+str(d2))\n",
    "        dlist.append(d1)\n",
    "\n",
    "    return np.sum([x*x for x in dlist])\n",
    "def distance_min(hist1, hist2, pca):\n",
    "    dim = len(hist1)\n",
    "    dlist=  []\n",
    "    for i in range(dim):\n",
    "        d1 = kl(hist1[i], hist2[i])\n",
    "        #d2 = kl(hist2[i], hist1[i])\n",
    "        #print(\"d = \"+str(d1))W_precision = 0.9\n",
    "        #print(\"d2 = \"+str(d2))\n",
    "        dlist.append(d1)\n",
    "\n",
    "    return np.min(dlist)\n",
    "\n",
    "def distance_min2(hist1, hist2, pca):\n",
    "    dim = len(hist1)\n",
    "    dlist=  []\n",
    "    for i in range(dim):\n",
    "        d1 = kl2(hist1[i], hist2[i])\n",
    "        #d2 = kl(hist2[i], hist1[i])\n",
    "        #print(\"d = \"+str(d1))W_precision = 0.9\n",
    "        #print(\"d2 = \"+str(d2))\n",
    "        dlist.append(d1)\n",
    "\n",
    "    return np.min(dlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequence(seq):\n",
    "    if not seq:\n",
    "        return seq\n",
    "\n",
    "    M = [None] * len(seq)    # offset by 1 (j -> j-1)\n",
    "    P = [None] * len(seq)\n",
    "\n",
    "    # Since we have at least one element in our list, we can start by \n",
    "    # knowing that the there's at least an increasing subsequence of length one:\n",
    "    # the first element.\n",
    "    L = 1\n",
    "    M[0] = 0\n",
    "\n",
    "    # Looping over the sequence starting from the second element\n",
    "    for i in range(1, len(seq)):\n",
    "        # Binary search: we want the largest j <= L\n",
    "        #  such that seq[M[j]] < seq[i] (default j = 0),\n",
    "        #  hence we want the lower bound at the end of the search process.\n",
    "        lower = 0\n",
    "        upper = L\n",
    "\n",
    "        # Since the binary search will not look at the upper bound value,\n",
    "        # we'll have to check that manually\n",
    "        if seq[M[upper-1]] < seq[i]:\n",
    "            j = upper\n",
    "\n",
    "        else:\n",
    "            # actual binary search loop\n",
    "            while upper - lower > 1:\n",
    "                mid = (upper + lower) // 2\n",
    "                if seq[M[mid-1]] < seq[i]:\n",
    "                    lower = mid\n",
    "                else:\n",
    "                    upper = mid\n",
    "\n",
    "            j = lower    # this will also set the default value to 0\n",
    "\n",
    "        P[i] = M[j-1]\n",
    "\n",
    "        if j == L or seq[i] < seq[M[j]]:\n",
    "            M[j] = i\n",
    "            L = max(L, j+1)\n",
    "\n",
    "    # Building the result: [seq[M[L-1]], seq[P[M[L-1]]], seq[P[P[M[L-1]]]], ...]\n",
    "    result = []\n",
    "    pos = M[L-1]\n",
    "    for _ in range(L):\n",
    "        result.append(seq[pos])\n",
    "        pos = P[pos]\n",
    "\n",
    "    return result[::-1]    # reversing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.00001\n",
    "def subsequence2(seq):\n",
    "    global alpha\n",
    "    if not seq:\n",
    "        return seq\n",
    "\n",
    "    M = [None] * len(seq)    # offset by 1 (j -> j-1)\n",
    "    P = [None] * len(seq)\n",
    "\n",
    "    # Since we have at least one element in our list, we can start by \n",
    "    # knowing that the there's at least an increasing subsequence of length one:\n",
    "    # the first element.\n",
    "    L = 1\n",
    "    M[0] = 0\n",
    "\n",
    "    # Looping over the sequence starting from the second element\n",
    "    for i in range(1, len(seq)):\n",
    "        # Binary search: we want the largest j <= L\n",
    "        #  such that seq[M[j]] < seq[i] (default j = 0),\n",
    "        #  hence we want the lower bound at the end of the search process.\n",
    "        lower = 0\n",
    "        upper = L\n",
    "\n",
    "        # Since the binary search will not look at the upper bound value,\n",
    "        # we'll have to check that manually\n",
    "        if seq[M[upper-1]] + alpha < seq[i]:\n",
    "            j = upper\n",
    "\n",
    "        else:\n",
    "            # actual binary search loop\n",
    "            while upper - lower > 1:\n",
    "                mid = (upper + lower) // 2\n",
    "                if seq[M[mid-1]] + alpha < seq[i]:\n",
    "                    lower = mid\n",
    "                else:\n",
    "                    upper = mid\n",
    "\n",
    "            j = lower    # this will also set the default value to 0\n",
    "\n",
    "        P[i] = M[j-1]\n",
    "\n",
    "        if j == L or seq[i] + alpha < seq[M[j]]:\n",
    "            M[j] = i\n",
    "            L = max(L, j+1)\n",
    "\n",
    "    # Building the result: [seq[M[L-1]], seq[P[M[L-1]]], seq[P[P[M[L-1]]]], ...]\n",
    "    result = []\n",
    "    pos = M[L-1]\n",
    "    for _ in range(L):\n",
    "        result.append(seq[pos])\n",
    "        pos = P[pos]\n",
    "\n",
    "    return result[::-1]    # reversing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "print(subsequence2([1, 4, 2, 5, 7, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 0.00043\n",
    "alpha = 0.0001\n",
    "\n",
    "def change_detection(data, window_size, slide_size):\n",
    "    threshold = 2*math.sqrt(window_size/slide_size)\n",
    "    \n",
    "    global alpha\n",
    "    \n",
    "    dim = len(data[0,:])\n",
    "    #print(\"dim = \"+ str(dim))\n",
    "    start_idx = 0\n",
    "    end_idx = window_size\n",
    "    #get reference \n",
    "    reference_window = data[0:window_size,:]\n",
    "    #print(reference_window[4999])\n",
    "    ref_hists = []\n",
    "    bin_edges = []\n",
    "    num_selected_components = 0\n",
    "    \n",
    "    \n",
    "    reference_window, bin_edges, dim, pca, ref_hists,num_selected_components = get_reference_window(reference_window, dim)\n",
    "    #alpha = x1/(len(bin_edges)-1)\n",
    "    print(\"Num bin = \" + str(len(bin_edges[0])))\n",
    "    \n",
    "    count_slide = 0\n",
    "    start_idx += slide_size\n",
    "    end_idx += slide_size\n",
    "    ref_end_idx = end_idx\n",
    "    distances = []\n",
    "    lis_sequence = []\n",
    "    changes = []\n",
    "    \n",
    "    possible_lis = 0\n",
    "    \n",
    "    while(end_idx < len(data)):\n",
    "        count_slide +=1\n",
    "        \n",
    "        current_window = data[start_idx: end_idx]\n",
    "        current_window = pca.transform(current_window)\n",
    "        #get current hist\n",
    "        current_hists = get_hists(current_window,num_selected_components, bin_edges)\n",
    "        #distance = distance2(current_hists,ref_hists, pca)\n",
    "        distance = distance_max(ref_hists,current_hists, pca)\n",
    "        #distance = distance_min(current_hists,ref_hists, pca)\n",
    "        #distance = distance_sum(current_hists,ref_hists, pca)\n",
    "        #distance = distance_sum_square(current_hists,ref_hists, pca)\n",
    "        if count_slide %50000 == 0:\n",
    "            print(\"Count slide = \"+ str(count_slide))\n",
    "            #print(\"distance = \"+ str(distance))\n",
    "        if start_idx > ref_end_idx:\n",
    "            distances.append(distance)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(distances) > window_size/slide_size:\n",
    "            del distances[0]\n",
    "        #print(distance)\n",
    "        if start_idx > ref_end_idx:\n",
    "            possible_lis +=1 \n",
    "        \n",
    "        if possible_lis > threshold:\n",
    "            lis = len(subsequence(distances))\n",
    "            possible_lis = lis\n",
    "            if start_idx > ref_end_idx and lis > threshold:\n",
    "                #print(\"Change at \"+ str(end_idx))\n",
    "                changes.append(end_idx)\n",
    "                #update reference\n",
    "                reference_window, bin_edges, dim, pca , ref_hists, num_selected_components= get_reference_window(data[start_idx: end_idx], dim)\n",
    "                ref_end_idx = end_idx\n",
    "                possible_lis = 0\n",
    "                distances = []\n",
    "        \n",
    "        start_idx += slide_size\n",
    "        end_idx += slide_size\n",
    "        #lis_sequence.append(lis)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #fig = plt.figure()\n",
    "    #plt.plot(lis_sequence)\n",
    "    #fig.show()\n",
    "    \n",
    "    return changes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_from_beginning = 0\n",
    "m_current = 0\n",
    "M_current = 0\n",
    "averaged_sc = 0\n",
    "def change_PHDT(curScore, Sigma, delta):\n",
    "    global step_from_beginning\n",
    "    global m_current\n",
    "    global M_current\n",
    "    global averaged_sc\n",
    "    averaged_sc = (averaged_sc * step_from_beginning + curScore)*1.0/(step_from_beginning+1)\n",
    "    m_current = m_current + averaged_sc  - curScore + delta\n",
    "    if abs(m_current) > M_current:\n",
    "        M_current = abs(m_current)\n",
    "    theta_t = Sigma * averaged_sc\n",
    "    if (M_current - m_current) > theta_t:\n",
    "        return True\n",
    "    \n",
    "    step_from_beginning += 1\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = 500\n",
    "delta = 0.0005\n",
    "def change_detection_PHDT(data, window_size, slide_size):\n",
    "    global step_from_beginning\n",
    "    global averaged_sc\n",
    "    global m_current \n",
    "    global M_current\n",
    "    \n",
    "    \n",
    "    changes = []\n",
    "    global Sigma\n",
    "    global delta\n",
    "    dim = len(data[0,:])\n",
    "    num_selected_components = 0\n",
    "    #print(\"dim = \"+ str(dim))\n",
    "    start_idx = 0\n",
    "    end_idx = window_size\n",
    "    ref_end_idx = end_idx\n",
    "    #get reference \n",
    "    reference_window = data[0:window_size,:]\n",
    "    #print(reference_window[4999])\n",
    "    \n",
    "    step_from_beginning = 0\n",
    "    averaged_sc = 0\n",
    "    \n",
    "    reference_window, bin_edges, dim, pca, ref_hists,num_selected_components = get_reference_window(reference_window, dim)\n",
    "    \n",
    "    count_slide = 0\n",
    "    start_idx += slide_size\n",
    "    end_idx += slide_size\n",
    "    distances = []\n",
    "    \n",
    "    \n",
    "    while(end_idx < len(data)):\n",
    "        count_slide +=1\n",
    "        \n",
    "            \n",
    "        current_window = data[start_idx: end_idx]\n",
    "        current_window = pca.transform(current_window)\n",
    "        #get current hist\n",
    "        current_hists = get_hists(current_window,num_selected_components, bin_edges)\n",
    "        distance = distance1(current_hists,ref_hists)\n",
    "        distances.append(distance)\n",
    "        \n",
    "        if count_slide %50000 == 0:\n",
    "            #print(\"Count slide = \"+ str(count_slide))\n",
    "            #print(\"averaged_sc = \"+ str(averaged_sc))\n",
    "            #print(\"Distance = \" + str(distance))\n",
    "            #print(\"M = \"+str(M_current))\n",
    "            #print(\"m = \"+ str(m_current))\n",
    "            pass\n",
    "        if(start_idx > ref_end_idx and  change_PHDT(distance, Sigma, delta)):\n",
    "            #print(\"Change at : \"+ str(end_idx))\n",
    "            \n",
    "            averaged_sc = 0\n",
    "            M_current = 0\n",
    "            m_current = 0\n",
    "            step_from_beginning = 0\n",
    "            changes.append(end_idx)\n",
    "            start_idx = end_idx\n",
    "            end_idx = start_idx + window_size\n",
    "            #update reference\n",
    "            reference_window, bin_edges, dim, pca , ref_hists, num_selected_components= get_reference_window(data[start_idx: end_idx], dim)\n",
    "            ref_end_idx = end_idx\n",
    "        end_idx += slide_size\n",
    "        start_idx += slide_size\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(ground_truth, result, decay_factor, window_size):\n",
    "    scores = [0 for x in result]\n",
    "    for i in range(len(result)):\n",
    "        #check if result[i] is the soonest detected change point in a distribution \n",
    "        rs = result[i]\n",
    "        #----find the closest left ground_truth\n",
    "        left = -1\n",
    "        for j in range(len(ground_truth)):\n",
    "            if ground_truth[j] <= rs and (i == 0 or (i > 0 and result[i-1] < ground_truth[j])) :\n",
    "                left = ground_truth[j]\n",
    "        if left != -1:\n",
    "            scores[i] =  np.exp(-decay_factor*(int((rs - left)/window_size)))\n",
    "    #print(scores)\n",
    "    w_precision = np.sum(scores)/len(result)\n",
    "    w_recall = np.sum(scores)/len(groud_truth)\n",
    "    w_f1 = 2*w_precision*w_recall/(w_precision+w_recall)\n",
    "    #print(\"W_precision = \"+str(w_precision))\n",
    "    #print(\"W_Recall = \"+ str(w_recall))\n",
    "    #print(\"W_f1 = \"+str(w_f1))\n",
    "    return w_precision, w_recall, w_f1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(groudtruths, all_outliers):\n",
    "    scores = []\n",
    "    for i in range(len(groudtruths)):\n",
    "        ground = groudtruths[i]\n",
    "        outliers = all_outliers[i]\n",
    "        #if (len(ground) != len(outliers)):\n",
    "            #print(\"Different length!!!!!!\")\n",
    "        score = 0\n",
    "        for k in range(len(outliers)):\n",
    "            if outliers[k] in ground:\n",
    "                score +=1\n",
    "        precision, recall = 0,0\n",
    "        if len(outliers) > 0:\n",
    "            precision = score *1.0/len(outliers)\n",
    "        recall = score *1.0/len(ground)\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2* precision * recall / (precision + recall)\n",
    "            scores.append(f1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return np.sum(scores)/len(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate high dimensional data\n",
    "dim = 5\n",
    "groud_truth = []\n",
    "\n",
    "mean = []\n",
    "cov = []\n",
    "for i in range(dim):\n",
    "    mean.append(0.5)\n",
    "    cov.append([])\n",
    "for i in range(len(cov)):\n",
    "    cov[i]= []\n",
    "    for j in range(dim):\n",
    "        cov[i].append(0)\n",
    "    for j in range(len(cov[i])):\n",
    "        if j == i:\n",
    "            cov[i][j] = 0.04\n",
    "        else:\n",
    "            cov[i][j] = 0.02\n",
    "num_change_dimension = 1\n",
    "\n",
    "def generate_dataset_high_dim(distribution_type, dim, length_chunk, mean, cov, change_value, change_type,change_param, num_chunk):\n",
    "    global num_change_dimension\n",
    "    #change mean\n",
    "    out = []\n",
    "    ground_truth = []\n",
    "    if distribution_type == \"gauss\":\n",
    "        for i in range(num_chunk):\n",
    "            generated_length_chunk = random.randint(length_chunk,length_chunk * 10)\n",
    "            x = np.random.multivariate_normal(mean, cov, generated_length_chunk)\n",
    "            if change_param == \"mean\":\n",
    "                if change_type == \"sudden\":\n",
    "                    for j  in range(num_change_dimension):\n",
    "                        mean[j] += change_value\n",
    "            \n",
    "            elif change_param == \"correlation\":\n",
    "                if change_type == \"sudden\":\n",
    "                    k = random.randint(0, dim-1)\n",
    "                    j = random.randint(0, dim-1)\n",
    "                    while j == k:\n",
    "                        j = random.randint(0, dim-1)\n",
    "                    if j != k:\n",
    "\n",
    "                        if cov[j][k]+change_value * cov[j][j] < cov[j][j]:\n",
    "                            cov[j][k] += change_value * cov[j][j]\n",
    "                            cov[k][j] = cov[j][k]\n",
    "                        else :\n",
    "                            cov[j][k] = 0.5 * cov[j][j]\n",
    "                            cov[k][j] = cov[j][k]    \n",
    "                    \n",
    "                    print(cov)\n",
    "            if i > 0:\n",
    "                groud_truth.append(len(out))\n",
    "            if change_type == \"sudden\" or len(out) == 0:\n",
    "                for xi in x:\n",
    "                    out.append(xi)\n",
    "        \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
